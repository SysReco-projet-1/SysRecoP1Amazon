# Tâche 3.5.1 : Modèles de référence (Baselines)

## Description

Cette tâche implémente deux modèles de référence simples pour la prédiction des évaluations :

1. **Baseline 1 - Moyenne globale** : Prédit la moyenne globale de toutes les évaluations
   - Formule : `r̂_u,i = r̄`
   - Où `r̄` est la moyenne de toutes les évaluations dans l'ensemble d'entraînement

2. **Baseline 2 - Moyenne par livre** : Prédit la moyenne des évaluations pour chaque livre
   - Formule : `r̂_u,i = r̄_i`
   - Où `r̄_i` est la moyenne des évaluations du livre `i`
   - Si le livre n'est pas dans l'ensemble d'entraînement, utilise `r̄`

## Métriques d'évaluation

Les deux modèles sont évalués avec :
- **RMSE** (Root Mean Squared Error) : `√(1/|T| Σ(r_u,i - r̂_u,i)²)`
- **MAE** (Mean Absolute Error) : `1/|T| Σ|r_u,i - r̂_u,i|`
- **Temps d'exécution** : Temps de calcul en secondes

## Structure des fichiers

```
step_3_5_1/
├── 3.5.1_baselines.py          # Script principal
└── 3.5.1_README.md             # Ce fichier
```

## Utilisation

### Script Python

```bash
python step_3_5_1/3.5.1_baselines.py
```

## Prérequis

Les fichiers de données suivants doivent exister :
- `outputs/splits/train_amazon_books_sample_active_users.csv`
- `outputs/splits/test_amazon_books_sample_active_users.csv`
- `outputs/splits/train_amazon_books_sample_temporal.csv`
- `outputs/splits/test_amazon_books_sample_temporal.csv`

Ces fichiers sont générés par la tâche 3.1.3 (Prétraitement).

## Sorties

Le script génère les fichiers suivants dans `outputs/step_3_5_1/3.5.1_baselines/` :
- `baselines_results_50k_active_users.json` : Résultats pour le dataset 50k utilisateurs actifs
- `baselines_results_temporal.json` : Résultats pour le dataset temporel

Chaque fichier JSON contient :
- Statistiques du dataset (taille train/test, nombre d'utilisateurs/livres)
- Résultats détaillés pour chaque baseline (RMSE, MAE, temps)
- Amélioration relative de la moyenne par livre vs moyenne globale

## Interprétation des résultats

### RMSE vs MAE
- **RMSE** pénalise davantage les grandes erreurs (au carré)
- **MAE** traite toutes les erreurs de manière égale

### Comparaison des baselines
- La **moyenne par livre** devrait généralement mieux performer que la **moyenne globale**
- L'amélioration indique la capacité du modèle à capturer les différences entre livres
- Un faible gain suggère que les livres ont des évaluations similaires en moyenne

### Utilisation comme référence
Ces baselines servent de point de comparaison pour les modèles plus complexes (k-NN, factorisation matricielle, etc.). Un bon modèle de recommandation doit significativement surpasser ces baselines simples.
