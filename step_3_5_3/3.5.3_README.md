# Tâche 3.5.3 : Analyse des résultats

## Description

Cette tâche fournit une analyse complète et approfondie de tous les modèles de prédiction implémentés dans la tâche 3.5 (baselines et k-NN). Elle répond aux quatre questions clés du sujet :

1. **Tableau comparatif** : Comparaison de tous les modèles (RMSE, MAE, temps)
2. **Analyse de performance** : Impact de k, comparaison des similarités, améliorations
3. **Compromis précision/temps** : Analyse coût computationnel vs précision
4. **Analyse d'erreurs** : Erreurs par type de livre, exemples de prédictions erronées

## Structure des fichiers

```
step_3_5_3/
├── 3.5.3_analysis.py           # Script d'analyse complète
└── 3.5.3_README.md             # Ce fichier
```

## Utilisation

### Script Python

```bash
python step_3_5_3/3.5.3_analysis.py
```

## Prérequis

Les fichiers suivants doivent exister :
- `outputs/step_3_5_1/3.5.1_baselines/baselines_results_50k_active_users.json`
- `outputs/step_3_5_2/3.5.2_knn/knn_optimization_50k_active_users.json`
- `outputs/splits/train_amazon_books_sample_active_users.csv`
- `outputs/splits/test_amazon_books_sample_active_users.csv`

Ces fichiers sont générés par les tâches 3.5.1 (Baselines) et 3.5.2 (k-NN).

## Sorties générées

### Fichiers JSON
- `comprehensive_comparison_table.csv` : Tableau comparatif complet de tous les modèles
- `performance_analysis.json` : Analyse détaillée des performances
- `error_analysis.json` : Analyse des erreurs par type de livre
- `final_synthesis.json` : Synthèse finale avec recommandations

### Graphiques
- `precision_time_tradeoff.png` : Compromis précision vs temps de calcul
- `error_analysis_by_popularity.png` : Distribution des erreurs par popularité de livre

## Analyses fournies

### 1. Tableau comparatif complet

Compare tous les modèles testés :
- 2 baselines (moyenne globale, moyenne par livre)
- 15 configurations k-NN (5 valeurs de k × 3 similarités)

Métriques : RMSE, MAE, Temps d'exécution

### 2. Analyse de performance

#### 2.1 Amélioration du k-NN vs baselines
- Calcul du % d'amélioration RMSE et MAE
- Comparaison avec les deux baselines
- Conclusion sur la significativité de l'amélioration

#### 2.2 Impact de k sur les performances
- Analyse de l'évolution RMSE/MAE en fonction de k
- Observations par mesure de similarité :
  - **Pearson** : Performance s'améliore avec k
  - **Cosinus/Jaccard** : Performance se dégrade légèrement avec k élevé

#### 2.3 Comparaison des mesures de similarité
- Statistiques (moyenne, min) par similarité
- **Résultat** : Pearson performe le mieux
- **Raison** : Centre les données → élimine les biais utilisateurs

### 3. Compromis précision/temps

#### 3.1 Comparaison temps de calcul
- Configuration la plus rapide vs la plus lente
- Ratio temps/précision

#### 3.2 Vaut-il le coût computationnel?
- Analyse du rapport amélioration/temps
- Contexte production vs recherche

#### 3.3 Stratégies d'accélération
Liste de 7 stratégies pour accélérer le k-NN :
1. Approximation des voisins (LSH, ANNOY, FAISS)
2. Réduire k (compromis acceptable)
3. Pré-calcul et cache des similarités
4. Parallélisation
5. Échantillonnage des candidats
6. Utilisation de GPU
7. Quantification (float16)

### 4. Analyse d'erreurs

#### 4.1 Erreurs par type de livre
Catégorisation des livres par popularité :
- **Niche** : < 25e percentile
- **Modéré** : 25-50e percentile
- **Populaire** : 50-75e percentile
- **Très populaire** : > 75e percentile

#### 4.2 Observation clé
**Les livres populaires sont mieux prédits que les livres de niche**

Raison : Plus de données d'entraînement → moyenne plus fiable

#### 4.3 Exemples de prédictions erronées
- Top 10 pires prédictions
- Analyse des causes :
  1. Livres peu populaires
  2. Utilisateurs atypiques
  3. Ratings extrêmes vs moyenne modérée

### 5. Synthèse finale

Résumé structuré avec :
- **5 résultats clés** identifiés
- **5 recommandations** pratiques
- **Meilleur modèle** : k-NN (k=100, Pearson)
  - RMSE : ~0.631
  - Amélioration : ~35% vs baseline

## Résultats attendus

### Amélioration typique du k-NN

Sur le dataset Amazon Books (50k utilisateurs actifs) :
- **RMSE** : Réduction de ~30-40% vs baseline moyenne par livre
- **MAE** : Réduction de ~40-45% vs baseline moyenne par livre

### Meilleure configuration

- **k** : 100 (pour Pearson)
- **Similarité** : Pearson
- **Raison** : Centre les données, élimine biais utilisateurs

### Temps d'exécution

- **Baselines** : < 10 secondes
- **k-NN** : 10-90 secondes (selon k et similarité)
- **Ratio** : 10-15x plus lent, mais amélioration significative

## Interprétation des résultats

### Pourquoi Pearson performe mieux?

**Pearson centre les données** : `r - r̄`

Cela élimine les biais utilisateurs :
- Utilisateur sévère (notes 1-3) vs généreux (notes 3-5)
- Pearson compare les **écarts à la moyenne**, pas les valeurs absolues

**Cosinus** ne centre pas → sensible aux échelles de notation

**Jaccard** ignore les valeurs → perd l'information de rating

### Pourquoi k=100 pour Pearson?

Plus de voisins = prédictions plus stables et robustes

Pour Pearson, augmenter k :
- ✓ Réduit la variance (plus de données)
- ✓ Améliore la précision (moyennes plus fiables)
- ✗ Augmente le temps de calcul

Pour Cosinus/Jaccard, k élevé peut dégrader la performance :
- Inclut des voisins moins similaires
- Dilue le signal des vrais voisins proches

### Livres populaires vs niche

**Populaires** : Beaucoup d'évaluations → moyenne fiable → bonnes prédictions

**Niche** : Peu d'évaluations → moyenne instable → prédictions moins précises

**Solution** : Techniques de cold start (factorisation matricielle, content-based)

## Recommandations finales

### Pour ce projet
1. **Utiliser k-NN (k=100, Pearson)** pour la meilleure précision
2. **Documenter le compromis** précision/temps dans le rapport
3. **Analyser les erreurs** par type de livre pour insights

### Pour la production
1. **Pré-calculer les similarités** (coût one-time)
2. **Utiliser k=30-50** pour bon compromis
3. **Implémenter approximation** (LSH) pour très grande échelle
4. **Combiner avec d'autres approches** (factorisation matricielle)

## Utilisation dans le rapport

Ce script génère tous les éléments nécessaires pour la section 3.5.3 du rapport :
- ✓ Tableau comparatif (CSV exportable)
- ✓ Graphiques professionnels (PNG haute résolution)
- ✓ Analyses quantitatives (JSON avec métriques)
- ✓ Synthèse structurée (recommandations)

Vous pouvez directement intégrer ces résultats dans votre rapport PDF.
